---
title: "Case_Study_4_Code"
author: "Brian Gaither, Sean Mcwhirter, Andrew Mejia, Sabrina Purvis"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 6
  github_document:
    toc: yes
    toc_depth: 6
  word_document:
    toc: yes
    toc_depth: '6'
always_allow_html: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, include=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(tm)
library(tidyverse)
library(randomForest)
library(caret)
library(knitr)
library(kableExtra)
library(XML)
library(stringr)
library(dplyr)
library(corrplot)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(dplyr)
library(ggraph)
library(igraph)
library(e1071)
```

```{r}
file_ext = "C:/Users/seans/Desktop/DS7333/CASE_STUDY_6/"
data = load(paste0(file_ext, "data.Rda"))
data = cbind(Email_Loc = rownames(emailDFrp), emailDFrp)
rownames(data) = 1:nrow(data) 
rand_seed = 42
```

```{r}
Descriptions = c('Multi-Level Factor Email Name', 'Binary Factor is Spam T/F', 'Binary Factor isRe ** T/F' , 'Binary Factor has Underscore T/F' , 
                 'Binary Factor Priority T/F', 'Binary Factor is Replying T/F', 'Binary Factor Sorted Received ** T/F', 
                 'Binary Factor SubPunc **T/F', 'Binary Factor Multi Part Text T/F', 'Binary Factor Has Image in Email T/F', 
                 'Binary Factor is PGPsinged ** T/F', 'Binary Factor is SubSpamWords ** T/F', 'Binary Factor No Host ** T/F', 
                 'Binary Factor numEnd** T/F', 'Binary Factor is Yelling T/F', 'Binary Factor is Orignal Message T/F', 
                 'Binary Factor is Dear** T/F', 'Binary Factor is Wrote ** T/F', 'Email Line Count', 
                 'Body Character Count', 'SubExc ** Count', 'Sub Queues ** Count', 'Number of ATT **', 'Number of Receipts **', 
                 'Number of Caps **' , 'Hour **', 'PerHTML **' , 'SubBlanks **', 'Fowards **', 'Average word Length', 
                 'Number of Dlr **' )
emailTableInfo = data.frame(Feature = names(data),  Type = sapply(data, class), row.names = NULL, Descriptions = Descriptions)
emailTableInfo
```
### Counts of NA
```{r}
na_df = data[rowSums(is.na(data)) > 0,]
dim(na_df)
head(na_df, n = 10)
```

### Drop NAs 
```{r}
data = data[complete.cases(data), ]
row.names(data) = NULL 
```


```{r}
ggplot(data, aes(x=isSpam)) +
  geom_bar(fill = "grey41")+
  ggtitle("Spam vs. Non-Spam Split")
```


```{r}
ggplot(data = data) +
  geom_count(mapping = aes(x = isSpam, y = isRe))+
  ggtitle("Spam vs. isRe count")
```

```{r}
ggplot(data = data) +
  geom_count(mapping = aes(x = isSpam, y = priority))+
  ggtitle("Spam vs. mail priority count")
```


```{r}
data %>% 
  count(isInReplyTo, isSpam) %>%  
  ggplot(mapping = aes(x = isInReplyTo, y = isSpam)) +
    geom_tile(mapping = aes(fill = n)) + ggtitle("Pairwise isSpam vs isInReplyTo")
```

#80/20 train test stratified split 
#https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/createDataPartition
```{r}

set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$isSpam,
  p = .80,
  list = FALSE
)
train = emailDFrp[train_partition,]
test =  emailDFrp[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```
```{r}
set.seed(rand_seed)
# create baseline random forest model
random_forest_1 <- randomForest(isSpam ~., data = train, ntree = 50, importance=TRUE, na.action = na.roughfix, maxnodes = 10)
preds_1 = predict(random_forest_1,test)
confusionMatrix(preds_1,test$isSpam)
```

```{r}
#Random forest tuning

set.seed(rand_seed)
# create baseline random forest model
random_forest_1 <- randomForest(isSpam ~., data = train, ntree = 500, importance=TRUE, na.action = na.roughfix, maxnodes = 20)
preds_1 = predict(random_forest_1,test)
confusionMatrix(preds_1,test$isSpam)
```


#Variable importance plot
#https://rdrr.io/cran/randomForest/man/varImpPlot.html
```{r, fig.dim=c(12,14)}
set.seed(rand_seed)
varImpPlot(random_forest_1, cex = .7, main = "Variable Importance",pt.cex = 1,color = 'grey41',frame.plot = FALSE,lcolor = 'black')
```

############################################################################################################################################
############################################################################################################################################
############################################################################################################################################

```{r}
#keeping most effective features (gini)

data2 = data %>% select(isSpam, perCaps, perHTML, forwards, bodyCharCt, numEnd, numLines, subBlanks, isInReplyTo, isRe, numDlr, subSpamWords, numRec)

#stratified split 
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data2$isSpam,
  p = .80,
  list = FALSE
)
train2 = emailDFrp[train_partition,]
test2 =  emailDFrp[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)

```

```{r}
head(data2)
```


```{r}

set.seed(rand_seed)
# create baseline random forest model
random_forest_2 <- randomForest(isSpam ~., data = train2, ntree = 50, importance=TRUE, na.action = na.roughfix, maxnodes = 10)
preds_2 = predict(random_forest_2,test2)
confusionMatrix(preds_2,test2$isSpam, positive = "T")
```

After actually reading up on Random Forest, I now realize that dropping unimportant features doesn't do anything.




Looping through some values for our parameters to see if that helps

```{r}
#loop for ntree

#blank df
#results <- data.frame(ntree=integer(0), accuracy=numeric(0), sensitivity=numeric(0), specificity=numeric(0))

x <- c(50, 100, 200, 400, 600, 1000)

for (val in x) {
  random_forest <- randomForest(isSpam ~., data = train, ntree = val, importance=TRUE, na.action = na.roughfix, maxnodes = 10)
  preds = predict(random_forest,test)
  sens = sensitivity(preds,test$isSpam)
  spec = specificity(preds,test$isSpam)
  print(val)
  print(sens)
  print(spec)
}

```



```{r}
#loop for maxnodes

#blank df
#results <- data.frame(ntree=integer(0), accuracy=numeric(0), sensitivity=numeric(0), specificity=numeric(0))

x <- c(10, 50, 100, 150, 200, 400, 800)

for (val in x) {
  random_forest <- randomForest(isSpam ~., data = train, ntree = 400, importance=TRUE, na.action = na.roughfix, maxnodes = val)
  preds = predict(random_forest,test)
  sens = sensitivity(preds,test$isSpam)
  spec = specificity(preds,test$isSpam)
  print(val)
  print(sens)
  print(spec)
}

```

It looks like we have some promising results between 200-800.  Testing 400 for now

```{r}
#Random forest with optimal ntree and maxnodes (400 & 400)

set.seed(rand_seed)
# create baseline random forest model
random_forest_1 <- randomForest(isSpam ~., data = train, ntree = 400, importance=TRUE, na.action = na.roughfix, maxnodes = 400)
preds_1 = predict(random_forest_1,test)
confusionMatrix(preds_1,test$isSpam)
```

#Variable importance plot
#https://rdrr.io/cran/randomForest/man/varImpPlot.html
```{r, fig.dim=c(12,14)}
set.seed(rand_seed)
varImpPlot(random_forest_1, cex = .7, main = "Variable Importance",pt.cex = 1,color = 'grey41',frame.plot = FALSE,lcolor = 'black')
```

It looks like PerCaps and perHTML both affect both accuracy and gini significantly

