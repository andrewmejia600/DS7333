---
title: "Case_Study_4_Code"
author: "Brian Gaither, Sean Mcwhirter, Andrew Mejia, Sabrina Purvis"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 6
  github_document:
    toc: yes
    toc_depth: 6
  word_document:
    toc: yes
    toc_depth: '6'
always_allow_html: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rpart)
library(tm)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(doSNOW)
library(parallel)
```

Build a function to produce the model results and confusion matrix
```{r}
predictandCM<- function(theModel,data, modeltype)
{
  pred <-predict(theModel,data,type=modeltype)
  confusionMatrix(pred, reference=data$isSpam, positive = 'T')
}
```

```{r}
file_ext = "C:/Users/blgai/OneDrive/Documents/School/SMU/Courses/Spring 2021/Quantifying The World/Case Study 6/data/"
data = load(paste0(file_ext, "data.Rda"))
```

```{r}
rownames(emailDFrp) <- NULL
head(emailDFrp)
```

```{r}
dim(emailDFrp)
```
which columns have missing values by target
```{r}
na_df = data.frame(rowsum(+(is.na(emailDFrp)), emailDFrp$isSpam))
na_df[, colSums(na_df != 0) > 0]
```


Let's see what the proportion of spam to ham is
```{r}
emailDFrp %>% count(isSpam) 
```

We'll drop the rows with NAs
```{r}
emailDFrp = emailDFrp[complete.cases(emailDFrp), ]
```



let's create a stratified training and test set to ensure proper proportion of the target variable
```{r}
train.index = createDataPartition(emailDFrp$isSpam, p=.7, list=FALSE)
train = emailDFrp[train.index,]
test = emailDFrp[-train.index,]
```

Check the dimensions of train and test
```{r}
dim(train)
dim(test)
```
check the proportion of target in train and test
```{r}
train %>% count(isSpam) 
test %>% count(isSpam) 
```


```{r}
CART_Fit = rpart(isSpam~., data=train, method='class')

fancyRpartPlot(fit,cex=.5)
```


```{r}
predictandCM(CART_Fit,train,"class")

```



k-fold cross validation
```{r}
set.seed(1234)
train.control = trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)
```


```{r}
fit_cv = train(isSpam~., data=train, method = "rpart2", tuneLength = 6, trControl = train.control, metric = "ROC")
```


```{r}
fit_cv
```


```{r}
plot(fit_cv)
```


```{r}

modelLookup("xgbTree")
```

Let's use XGBoost and perform a grid search
```{r}
library(doSNOW)
numberOfCores = detectCores()
cl = makeCluster(numberOfCores, type = "SOCK")
registerDoSNOW(cl)

set.seed(1234)
train.control = trainControl(
  method = "repeatedcv",
  number = 5, # 5-fold cross validation
  repeats = 3, # repeated three times
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)

tune.gridxgb <- expand.grid(eta = c(0.05,0.3, 0.075), # Shrinkage 
                      nrounds = c(50, 75, 100),  # Boosting Iterations
                      max_depth = 4:7,  # Max Tree Depth
                      min_child_weight = c(2.0, 2.25), # Minimum Sum of Instance Weight 
                      colsample_bytree = c(0.3, 0.4, 0.5), # Subsample Ratio of Columns
                      gamma = 0, # Minimum Loss Reduction
                      subsample = 1)  # Subsample Percentage

xgBoostGrid = train(isSpam~.,
                    data=train,
                    method="xgbTree",
                    tuneGrid = tune.gridxgb,
                    trControl = train.control)


stopCluster(cl)

```


```{r}
plot(xgBoostGrid)
```
Check the parameters for the model that had best results
```{r}
xgBoostGrid$bestTune
```
Convert labels to 0 and 1
```{r}
lvl = c('F','T')
tmp = as.character(train$isSpam)
tmp = as.numeric(factor(tmp,levels=lvl))-1
train$isSpam = tmp
tmp = as.character(test$isSpam)
tmp = as.numeric(factor(tmp,levels=lvl))-1
test$isSpam = tmp
```

Build final model using best parameters
```{r}
#train_T = train
#train_T$myLab <- with(train_T, ifelse(isSpam == "T", 1, 0))
#train_T = train_T["myLab"]
y = data.matrix(train[,1])
x = data.matrix(train[,-1])

xgbFinal = xgboost(data=x, label= y, nrounds = 100, max_depth = 7, eta = 0.3, gamma = 0, colsample_bytree = 0.5, min_child_weight = 2, subsample = 1, objective="binary:logistic")

```

Check results of final model
```{r}

#make the prediction
pred = predict(xgbFinal, data.matrix(test[,-1]))

#to make confusion matrix based on T and F target values
pred_label <- lvl[as.numeric(pred>.5)+1]
actual_label = lvl[as.numeric(test$isSpam)+1]
#create a confusion matrix
table(pred_label, actual_label)

```

```{r}
#now let's try to use confusionMatrix from the caret package with threshold set at .5
pred_label = as.numeric(pred>.5)
#create confusion matrix
confusionMatrix(factor(pred_label), factor(test$isSpam) , positive = "1")
```

```{r}

library(ROCR)
#generage ROC curve
myPred = prediction(pred,test$isSpam)
perf <-performance(myPred,"tpr","fpr")
#calculate AUC
auc = performance(myPred, measure="auc")
auc = auc@y.values[[1]]

#plot the curve
plot(perf,main=paste0("XGBoost ROC curve: AUC= ",auc), xlim=c(0,0.1), ylim=c(.8,1),colorize=TRUE)

```

We want to optimize for minimal false positives to ensure important messages get through without being blocked, this threshold drops the FPs to 6 from 14, however we miss 126 spam messages in this test set versus missing only 48 with a threshold of .5
```{r}
#now let's try to use confusionMatrix from the caret package with threshold set to minimize falses based on ROC curve analysis
pred_label = as.numeric(pred>.875)
#create confusion matrix
confusionMatrix(factor(pred_label), factor(test$isSpam) , positive = "1")
```

Feature importance
```{r}
xgb.importance(feature_names = colnames(train[,-1]), model = xgbFinal, data=train[,-1], label=train[,1])
```

Plot tree
interpretation from answer 12 on stack exchange here:  https://stats.stackexchange.com/questions/395697/what-is-an-intuitive-interpretation-of-the-leaf-values-in-xgboost-base-learners
When performing a binary classification task, by default, XGBoost treats it as a logistic regression problem. As such the raw leaf estimates seen here are log-odds and can be negative. The result from each individual tree are indeed combined together, but they are not probabilities (yet) but rather the estimates of the score before performing the logistic transformation done when performing logistic regression. For that reason the individual as well as the combined estimates show can naturally be negative; the negative sign simply implies "less" chance.  The leaves contain the estimates from their respective base-learner on the domain of the function where the gradient boosting procedure takes place. For the presented binary classification task, the link used is the logit so these estimates represent log-odds; in terms of log-odds, negative values are perfectly normal. To get probability estimates we simply use the logistic function, which is the inverse of the logit function.
```{r}
xgb.plot.tree(model=xgbFinal, trees = 0, plot_width = 900, plot_height = 900, show_node_id = TRUE)

```

export to .pdf for easier viewing and interpreting
```{r}
library(DiagrammeR)
gr = xgb.plot.tree(model=xgbFinal, trees = 0, render = FALSE)
export_graph(gr,'tree.pdf',width=1500, height=1500)
```






